{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Process Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import metrics\n",
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "matplotlib.use('agg') \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import data from Cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do data preprocessing on our cifar10 data. We will normalize our data by divided them by 225, which can make the image value change from 1-225 to 0-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standard normalize the inputs\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: One-hot encode outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the CNN model with batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add several layers in our structure, which include convoluntional layers, max pooling layers, batch normalization layers. Batch Normalization means each layer takes a mini batch data set and stores the running average of those means and standard deviations. This layer helps in trainning a deeper neural network. We aso use max pooling, but we only add two to three pooling layers since it may result in losing information features of the image. Then we set epoch = 40 and iliterate about 200000 times in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 32, 32)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 32, 32)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 64, 16, 16)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 8, 8)          32        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               1638800   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                4010      \n",
      "=================================================================\n",
      "Total params: 1,735,066\n",
      "Trainable params: 1,726,762\n",
      "Non-trainable params: 8,304\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Construct the model which include convolutional layer, max pooling layer and batch normalization layer.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(3, 32, 32), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(400))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model and set epochs = 40.\n",
    "epochs = 40\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Fit the model and calculate the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model using training data with size 40,000, and validate it using small batches (32) with a total of 10,000 due to GPU capacity. Test accuracy is calculated and below is the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      "50000/50000 [==============================] - 44s 876us/step - loss: 2.1526 - acc: 0.2917 - val_loss: 2.9448 - val_acc: 0.1888\n",
      "Epoch 2/40\n",
      "50000/50000 [==============================] - 42s 846us/step - loss: 1.6420 - acc: 0.4155 - val_loss: 1.5542 - val_acc: 0.4409\n",
      "Epoch 3/40\n",
      "50000/50000 [==============================] - 42s 839us/step - loss: 1.4399 - acc: 0.4857 - val_loss: 1.3172 - val_acc: 0.5205\n",
      "Epoch 4/40\n",
      "50000/50000 [==============================] - 42s 843us/step - loss: 1.3229 - acc: 0.5321 - val_loss: 1.8483 - val_acc: 0.4089\n",
      "Epoch 5/40\n",
      "50000/50000 [==============================] - 42s 843us/step - loss: 1.2452 - acc: 0.5587 - val_loss: 1.3718 - val_acc: 0.5154\n",
      "Epoch 6/40\n",
      "50000/50000 [==============================] - 41s 824us/step - loss: 1.1870 - acc: 0.5790 - val_loss: 1.3269 - val_acc: 0.5405\n",
      "Epoch 7/40\n",
      "50000/50000 [==============================] - 42s 839us/step - loss: 1.1415 - acc: 0.5981 - val_loss: 1.3136 - val_acc: 0.5393\n",
      "Epoch 8/40\n",
      "50000/50000 [==============================] - 42s 837us/step - loss: 1.0948 - acc: 0.6160 - val_loss: 1.1714 - val_acc: 0.5876\n",
      "Epoch 9/40\n",
      "50000/50000 [==============================] - 42s 843us/step - loss: 1.0650 - acc: 0.6253 - val_loss: 1.0703 - val_acc: 0.6244\n",
      "Epoch 10/40\n",
      "50000/50000 [==============================] - 42s 830us/step - loss: 1.0464 - acc: 0.6352 - val_loss: 1.1310 - val_acc: 0.6057\n",
      "Epoch 11/40\n",
      "50000/50000 [==============================] - 42s 831us/step - loss: 1.0256 - acc: 0.6399 - val_loss: 1.1053 - val_acc: 0.6121\n",
      "Epoch 12/40\n",
      "50000/50000 [==============================] - 42s 840us/step - loss: 1.0078 - acc: 0.6460 - val_loss: 1.0035 - val_acc: 0.6456\n",
      "Epoch 13/40\n",
      "50000/50000 [==============================] - 42s 835us/step - loss: 0.9927 - acc: 0.6523 - val_loss: 1.0648 - val_acc: 0.6271\n",
      "Epoch 14/40\n",
      "50000/50000 [==============================] - 42s 836us/step - loss: 0.9822 - acc: 0.6550 - val_loss: 1.0129 - val_acc: 0.6391\n",
      "Epoch 15/40\n",
      "50000/50000 [==============================] - 42s 841us/step - loss: 0.9747 - acc: 0.6577 - val_loss: 1.0685 - val_acc: 0.6198\n",
      "Epoch 16/40\n",
      "50000/50000 [==============================] - 42s 840us/step - loss: 0.9600 - acc: 0.6635 - val_loss: 1.0056 - val_acc: 0.6492\n",
      "Epoch 17/40\n",
      "50000/50000 [==============================] - 42s 832us/step - loss: 0.9573 - acc: 0.6659 - val_loss: 0.9671 - val_acc: 0.6564\n",
      "Epoch 18/40\n",
      "50000/50000 [==============================] - 42s 832us/step - loss: 0.9432 - acc: 0.6702 - val_loss: 1.1995 - val_acc: 0.5966\n",
      "Epoch 19/40\n",
      "50000/50000 [==============================] - 41s 823us/step - loss: 0.9381 - acc: 0.6737 - val_loss: 0.9720 - val_acc: 0.6568\n",
      "Epoch 20/40\n",
      "50000/50000 [==============================] - 42s 839us/step - loss: 0.9293 - acc: 0.6766 - val_loss: 0.9598 - val_acc: 0.6623\n",
      "Epoch 21/40\n",
      "50000/50000 [==============================] - 42s 834us/step - loss: 0.9234 - acc: 0.6778 - val_loss: 0.9904 - val_acc: 0.6572\n",
      "Epoch 22/40\n",
      "50000/50000 [==============================] - 42s 831us/step - loss: 0.9176 - acc: 0.6827 - val_loss: 0.9146 - val_acc: 0.6756\n",
      "Epoch 23/40\n",
      "50000/50000 [==============================] - 41s 830us/step - loss: 0.9097 - acc: 0.6833 - val_loss: 0.9339 - val_acc: 0.6682\n",
      "Epoch 24/40\n",
      "50000/50000 [==============================] - 41s 827us/step - loss: 0.9032 - acc: 0.6838 - val_loss: 0.9712 - val_acc: 0.6568\n",
      "Epoch 25/40\n",
      "50000/50000 [==============================] - 41s 827us/step - loss: 0.9043 - acc: 0.6837 - val_loss: 0.9218 - val_acc: 0.6730\n",
      "Epoch 26/40\n",
      "50000/50000 [==============================] - 42s 832us/step - loss: 0.8933 - acc: 0.6871 - val_loss: 0.8860 - val_acc: 0.6852\n",
      "Epoch 27/40\n",
      "50000/50000 [==============================] - 41s 820us/step - loss: 0.8950 - acc: 0.6868 - val_loss: 0.9375 - val_acc: 0.6689\n",
      "Epoch 28/40\n",
      "50000/50000 [==============================] - 42s 832us/step - loss: 0.8874 - acc: 0.6906 - val_loss: 0.9657 - val_acc: 0.6602\n",
      "Epoch 29/40\n",
      "50000/50000 [==============================] - 41s 825us/step - loss: 0.8819 - acc: 0.6921 - val_loss: 0.9543 - val_acc: 0.6641\n",
      "Epoch 30/40\n",
      "50000/50000 [==============================] - 42s 834us/step - loss: 0.8736 - acc: 0.6950 - val_loss: 0.9281 - val_acc: 0.6728\n",
      "Epoch 31/40\n",
      "50000/50000 [==============================] - 42s 832us/step - loss: 0.8701 - acc: 0.6961 - val_loss: 0.8985 - val_acc: 0.6823\n",
      "Epoch 32/40\n",
      "50000/50000 [==============================] - 41s 824us/step - loss: 0.8670 - acc: 0.6976 - val_loss: 0.9302 - val_acc: 0.6764\n",
      "Epoch 33/40\n",
      "50000/50000 [==============================] - 41s 830us/step - loss: 0.8625 - acc: 0.6984 - val_loss: 0.9280 - val_acc: 0.6793\n",
      "Epoch 34/40\n",
      "50000/50000 [==============================] - 41s 821us/step - loss: 0.8633 - acc: 0.6993 - val_loss: 0.9232 - val_acc: 0.6808\n",
      "Epoch 35/40\n",
      "50000/50000 [==============================] - 41s 823us/step - loss: 0.8561 - acc: 0.7019 - val_loss: 0.9204 - val_acc: 0.6763\n",
      "Epoch 36/40\n",
      "50000/50000 [==============================] - 41s 819us/step - loss: 0.8555 - acc: 0.7018 - val_loss: 0.9145 - val_acc: 0.6813\n",
      "Epoch 37/40\n",
      "50000/50000 [==============================] - 41s 825us/step - loss: 0.8502 - acc: 0.7025 - val_loss: 0.8976 - val_acc: 0.6860\n",
      "Epoch 38/40\n",
      "50000/50000 [==============================] - 41s 823us/step - loss: 0.8478 - acc: 0.7043 - val_loss: 0.9768 - val_acc: 0.6591\n",
      "Epoch 39/40\n",
      "50000/50000 [==============================] - 41s 829us/step - loss: 0.8431 - acc: 0.7053 - val_loss: 0.9118 - val_acc: 0.6819\n",
      "Epoch 40/40\n",
      "50000/50000 [==============================] - 41s 826us/step - loss: 0.8425 - acc: 0.7069 - val_loss: 0.9087 - val_acc: 0.6807\n",
      "Accuracy: 68.07%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=32)\n",
    "# Accuracy of the BN-model\n",
    "acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (acc[1]*100))\n",
    "#This framework is ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the result of our BN model is not as good as other implementations. But the batch normalization method indeed can improve our model and simplfy the computation complexity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
