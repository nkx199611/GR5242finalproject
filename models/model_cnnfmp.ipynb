{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network with Fractional Max Pooling#\n",
    "## Full Process Jupyter Notebook ##\n",
    "\n",
    "### Introduction ###\n",
    "This model is inspired by the all-convolutional network article, which is cited in the main report. In our cnnfmp model, we replace the conv2x2 layers with stride 2 with the fractional max pooling layers. The author of all-CNN model believes that pooling layers decrease information layers contain, so replacing max pooling layers with convolutional layers will be effective. In the results, all-cnn outperforms many methods except for fractional pooling method. We believe that fractional max pooling method has improved pooling. So we replace the stride-2 conv2x2 layers in all-CNN model with fractional max pooling layers to see whether it improve the performance of all-cnn model. For more information about fractional max pooling and all-cnn model, please refer to our report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step0：Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Data Processing\n",
    "we use CIFAR-10 dataset for python. To unpickle this data file, we first define help function. Then we load data from six batches. We then have four batches of training data of size 10000, one batch validation data of size 10000 and one batch of test data of size 10000. \n",
    "Then we need to modify the dimension of y labels. Right now of labels are in 1-d, we will change then to a 10-dimensional vector containing indicators for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "   \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.array([])\n",
    "y_train=np.array([])\n",
    "x_val=np.array([])\n",
    "y_val=np.array([])\n",
    "for i in range(1,5):\n",
    "    \n",
    "    batch = unpickle(\"/data/cifar-10-batches-py/data_batch_%d\"%(i))\n",
    "    if len(x_train)==0 & len(y_train)==0:\n",
    "        x_train = batch[b'data']\n",
    "        y_train = batch[b'labels']\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train, batch[b'data'])) \n",
    "        y_train = np.concatenate((y_train, batch[b'labels']))\n",
    "\n",
    "v_batch = unpickle(\"/data/cifar-10-batches-py/data_batch_5\")\n",
    "x_val = v_batch[b'data']\n",
    "y_val = v_batch[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train0 = np.empty((0,10), int)\n",
    "for y in y_train:\n",
    "    indi= [1 if i==y else 0 for i in range(0,10)]\n",
    "    y_train0 = np.append(y_train0, np.array([indi]), axis=0)\n",
    "y_val0 = np.empty((0,10), int)\n",
    "for y in y_val:\n",
    "    indi= [1 if i==y else 0 for i in range(0,10)]\n",
    "    y_val0 = np.append(y_val0, np.array([indi]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = unpickle(\"/data/cifar-10-batches-py/test_batch\")\n",
    "x_test = test_batch[b'data']\n",
    "y_test = test_batch[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test0 = np.empty((0,10), int)\n",
    "for y in y_test:\n",
    "    indi= [1 if i==y else 0 for i in range(0,10)]\n",
    "    y_test0 = np.append(y_test0, np.array([indi]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the data preprocessing part. We notice that there are many data augmentation methods. The one the all-CNN net author use is changing the 32x32x3 data into 126x126x3 aggressively. We tried this method, however, due to the GPU capacity(Tesla k80), we had a hard time training the model with data of dimension 126x126x3. So we use the original images size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Build The CNNFMP Model\n",
    "We replaced all the stride2 conv2x2 layers with fractional max pooling layers of pooling size sqrt(2) except for the last stride 2 conv2x2 layer. Because stride 2 conv2x2 layers are mainly used to reduce the dimensions of images, so we use fractional max pooling for same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logits(x):\n",
    "    \"\"\"\n",
    "    CNN-FMP network model\n",
    "    This function compute the logits for the layers, the input is a x data set with dimension [none:32*32*3]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #input x.shape = [,32*32*3]\n",
    "    x_image = tf.reshape(x,[-1,32,32,3])\n",
    "    \n",
    "    #define some constants, stack size for convolutional layers\n",
    "    n1=320\n",
    "    n2=640\n",
    "    n3=960\n",
    "    n4=1280\n",
    "    n5=1600\n",
    "    n6=1920\n",
    "\n",
    "    \n",
    "    #Block1\n",
    "    #cnn_1\n",
    "    W_conv1 = tf.get_variable('W_conv1', shape=[2, 2, 3, n1])\n",
    "    b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "    h_conv1 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME'), b_conv1))\n",
    "    #cnn_2\n",
    "    W_conv2 = tf.get_variable('W_conv2', shape=[2, 2, n1, n1])\n",
    "    b_conv2 = tf.get_variable('b_conv2', shape=[n1])\n",
    "    h_conv2 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(h_conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME'), b_conv2))\n",
    "    #fractional_max_pooling_1 (pooling size is sqrt(2)=1.414)\n",
    "    fmp_1=tf.nn.fractional_max_pool(h_conv2,pooling_ratio=[1.0, 1.414, 1.414, 1.0])[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    #block_2\n",
    "    #dropout rate = 0.1 \n",
    "    #cnn_3\n",
    "    W_conv3 = tf.get_variable('W_conv3', shape=[2, 2, n1, n2])\n",
    "    b_conv3 = tf.get_variable('b_conv3', shape=[n2])\n",
    "    h_conv3 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(fmp_1, W_conv3, strides=[1, 1, 1, 1], padding='SAME'), b_conv3))\n",
    "    #dropout_1\n",
    "    d_1=tf.nn.dropout(h_conv3,0.9)\n",
    "    #cnn_4\n",
    "    W_conv4 = tf.get_variable('W_conv4', shape=[2, 2, n2, n2])\n",
    "    b_conv4 = tf.get_variable('b_conv4', shape=[n2])\n",
    "    h_conv4 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_1, W_conv4, strides=[1, 1, 1, 1], padding='SAME'), b_conv4))\n",
    "    #dropout_2\n",
    "    d_2=tf.nn.dropout(h_conv4,0.9)\n",
    "    #fractional_max_pooling_2\n",
    "    fmp_2 = tf.nn.fractional_max_pool(d_2,pooling_ratio=[1.0, 1.414, 1.414, 1.0])[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #block_3\n",
    "    #dropout=0.2\n",
    "    #cnn_5\n",
    "    W_conv5 = tf.get_variable('W_conv5', shape=[2, 2, n2, n3])\n",
    "    b_conv5 = tf.get_variable('b_conv5', shape=[n3])\n",
    "    h_conv5 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(fmp_2, W_conv5, strides=[1, 1, 1, 1], padding='SAME'), b_conv5))\n",
    "    #dropout_3\n",
    "    d_3=tf.nn.dropout(h_conv5,0.8)\n",
    "    #cnn_6\n",
    "    W_conv6 = tf.get_variable('W_conv6', shape=[2, 2, n3, n3])\n",
    "    b_conv6 = tf.get_variable('b_conv6', shape=[n3])\n",
    "    h_conv6 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_3, W_conv6, strides=[1, 1, 1, 1], padding='SAME'), b_conv6))\n",
    "    #dropout_4\n",
    "    d_4=tf.nn.dropout(h_conv6,0.8)\n",
    "    #fractional_max_pooling_3\n",
    "    fmp_3 = tf.nn.fractional_max_pool(d_4,pooling_ratio=[1.0, 1.414, 1.414, 1.0])[0]\n",
    "    \n",
    "\n",
    "    \n",
    "    #block_4\n",
    "    #dropout=0.3\n",
    "    #cnn_7\n",
    "    W_conv7 = tf.get_variable('W_conv7', shape=[2, 2, n3, n4])\n",
    "    b_conv7 = tf.get_variable('b_conv7', shape=[n4])\n",
    "    h_conv7 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(fmp_3, W_conv7, strides=[1, 1, 1, 1], padding='SAME'), b_conv7))\n",
    "    #dropout_5\n",
    "    d_5=tf.nn.dropout(h_conv7,0.7)\n",
    "    #cnn_8\n",
    "    W_conv8 = tf.get_variable('W_conv8', shape=[2, 2, n4, n4])\n",
    "    b_conv8 = tf.get_variable('b_conv8', shape=[n4])\n",
    "    h_conv8 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_5, W_conv8, strides=[1, 1, 1, 1], padding='SAME'), b_conv8))\n",
    "    #dropout_6\n",
    "    d_6=tf.nn.dropout(h_conv8,0.7)\n",
    "    #fractional_max_pooling_4\n",
    "    fmp_4 = tf.nn.fractional_max_pool(d_6,pooling_ratio=[1.0, 1.414, 1.414, 1.0])[0]\n",
    "    \n",
    "    \n",
    "    #block_5\n",
    "    #dropout=0.4\n",
    "    #cnn_9\n",
    "    W_conv9 = tf.get_variable('W_conv9', shape=[2, 2, n4, n5])\n",
    "    b_conv9 = tf.get_variable('b_conv9', shape=[n5])\n",
    "    h_conv9 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(fmp_4, W_conv9, strides=[1, 1, 1, 1], padding='SAME'), b_conv9))\n",
    "    #dropout_7\n",
    "    d_7=tf.nn.dropout(h_conv9,0.6)\n",
    "    #cnn_10\n",
    "    W_conv10 = tf.get_variable('W_conv10', shape=[2, 2, n5, n5])\n",
    "    b_conv10 = tf.get_variable('b_conv10', shape=[n5])\n",
    "    h_conv10 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_7, W_conv10, strides=[1, 1, 1, 1], padding='SAME'), b_conv10))\n",
    "    #dropout_8\n",
    "    d_8=tf.nn.dropout(h_conv10,0.6)\n",
    "    #fractional_max_pooling_5\n",
    "    fmp_5 = tf.nn.fractional_max_pool(d_8,pooling_ratio=[1.0, 1.414, 1.414, 1.0])[0]\n",
    "   \n",
    "\n",
    "\n",
    "    #block_6\n",
    "    #dropout=0.5\n",
    "    #cnn_11\n",
    "    W_conv11 = tf.get_variable('W_conv11', shape=[2, 2, n5, n6])\n",
    "    b_conv11 = tf.get_variable('b_conv11', shape=[n6])\n",
    "    h_conv11 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(fmp_5, W_conv11, strides=[1, 1, 1, 1], padding='SAME'), b_conv11))\n",
    "    #dropout_9\n",
    "    d_9=tf.nn.dropout(h_conv11,0.5)\n",
    "    #cnn_12\n",
    "    W_conv12 = tf.get_variable('W_conv12', shape=[2, 2, n6, n6])\n",
    "    b_conv12 = tf.get_variable('b_conv12', shape=[n6])\n",
    "    h_conv12 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_9, W_conv12, strides=[1, 1, 1, 1], padding='SAME'), b_conv12))\n",
    "    #dropout_10\n",
    "    d_10=tf.nn.dropout(h_conv12,0.5)\n",
    "    #fractional_max_pooling_6\n",
    "    fmp_6 = tf.nn.fractional_max_pool(d_10,pooling_ratio=[1.0, 1.414, 1.414, 1.0])[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #lcnn with stride=2 \n",
    "    W_conv = tf.get_variable('W_conv', shape=[2, 2, n6, n6])\n",
    "    b_conv = tf.get_variable('b_conv', shape=[n6])\n",
    "    h_conv = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(fmp_6, W_conv, strides=[1, 2, 2, 1], padding='SAME'), b_conv))\n",
    "\n",
    "    # fc layer to logits 10\n",
    "    h_flat = tf.reshape(h_conv, [-1, 1*1*n6])\n",
    "    W_fc1 = tf.get_variable('W_fc1', shape=[1*1*n6, 10])\n",
    "    b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "    #layer of output\n",
    "    logits = tf.add(tf.matmul(h_flat, W_fc1), b_fc1, name='logits')\n",
    "    \n",
    "    return(logits)\n",
    "\n",
    "def compute_cross_entropy(logits, y):\n",
    "    \"\"\"compute the prediction and cross_entropy of model\"\"\"\n",
    "    # This function is used from the in-class example code\n",
    "    numerical_instability_example = 0\n",
    "    if numerical_instability_example:\n",
    "        y_pred = tf.nn.softmax(logits, name='y_pred') \n",
    "        cross_ent = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=[1]))\n",
    "    else:\n",
    "        sm_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits, name='cross_ent_terms')\n",
    "        cross_ent = tf.reduce_mean(sm_ce, name='cross_ent')\n",
    "    return cross_ent\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    \"compare prediction to labels(also from class example)\"\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Train, Validate and Test the CNN-FMP Model\n",
    "In this part, we also save all the summaries, which are displayed in Tensorboard.\n",
    "We train the model using small batches(100) due to the GPU capacity. Also, for validation set, we validate model using a 1000 random sample from the validation 10000 data set for the same reason. Test accuracy is calculated using batches of 1000 and their average is calculated below. This result is for models comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After step   0, valiation accuracy 0.0950\n",
      "After step 1000, valiation accuracy 0.3360\n",
      "After step 2000, valiation accuracy 0.4430\n",
      "After step 3000, valiation accuracy 0.5390\n",
      "After step 4000, valiation accuracy 0.5530\n",
      "After step 5000, valiation accuracy 0.5620\n",
      "After step 6000, valiation accuracy 0.5860\n",
      "After step 7000, valiation accuracy 0.5830\n",
      "After step 8000, valiation accuracy 0.5900\n",
      "After step 9000, valiation accuracy 0.6200\n",
      "After step 10000, valiation accuracy 0.6190\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0312ab2cbea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mtest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_test0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\rFinal test accuracy is{1:0.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "dir_name = 'logs/scratch04x/summary'\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 32*32*3], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "\n",
    "    \n",
    " \n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits(x)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
    "    \n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "        summary_writer_test = tf.summary.FileWriter(dir_name+'/test')\n",
    "        summary_writer_val = tf.summary.FileWriter(dir_name+'/val')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        batch=0\n",
    "        for i in range(10001):\n",
    "            \n",
    "             \n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                feed_dict={x: x_train[100*batch:100*(batch+1)], y: y_train0[100*batch:100*(batch+1)]})\n",
    "            batch=batch+1\n",
    "            if batch == 50:\n",
    "                batch=0\n",
    "            if i%10==0:\n",
    "                t=rd.sample(range(0,10000),1000)\n",
    "                summary_writer_train.add_summary(summary, i)\n",
    "                (val_accuracy, summary_t) = sess.run((accuracy,summary_op), {x:x_val[t], y:y_val0[t]})\n",
    "                summary_writer_val.add_summary(summary_t, i)\n",
    "                if i%1000 == 0:\n",
    "                    print(\"\\rAfter step {0:3d}, valiation accuracy {1:0.4f}\".format(i, val_accuracy), flush=True)\n",
    "        #calculate test error\n",
    "        test_ac=[]\n",
    "        for i in range(0,10):\n",
    "            acu = sess.run(accuracy,{x:x_test[1000*i:1000*(i+1)],y:y_test0[1000*i:1000*(i+1)]}）\n",
    "            test_ac.append(acu)\n",
    "        all_ac = sum(test_ac)*0.1\n",
    "        print(\"\\rFinal test accuracy is %.4f\"all_ac)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion ###\n",
    "According to the validation accuracy and test accuracy, it seems that cnn-fmp model doesn't outperform the original all-convolutional network. Compared to the input data dimension, our filter size is too big without expanding the data dimension, which creating many variables to calculate gradients, also might cause overfitting. According to our tensorboard output, we actually have over-fitting issues. Maybe we can work on the variables selection part when developing this model. Also, a better GPU might let us try to bigger dimension input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ac' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5e4223077c9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_ac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ac' is not defined"
     ]
    }
   ],
   "source": [
    "test_ac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
