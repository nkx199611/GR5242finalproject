{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduced_all_CNN Model\n",
    "## Full Process Jupyter Notebook ##\n",
    "\n",
    "### Introduction ###\n",
    "This model is inspired by the all-convolutional network article, which is cited in the main report. In this model, we removed the second half of convolutional layers for ALL_Convolutional Network. Some authors believe that a smaller network will work better on CIFAR-10. With this thought, we built an all_cnn model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step0: Upload Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Data Processing\n",
    "we use CIFAR-10 dataset for python. To unpickle this data file, we first define help function. Then we load data from six batches. We then have four batches of training data of size 10000, one batch validation data of size 10000 and one batch of test data of size 10000. \n",
    "Then we need to modify the dimension of y labels. Right now of labels are in 1-d, we will change then to a 10-dimensional vector containing indicators for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "   \n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=np.array([])\n",
    "y_train=np.array([])\n",
    "x_val=np.array([])\n",
    "y_val=np.array([])\n",
    "for i in range(1,5):\n",
    "    \n",
    "    batch = unpickle(\"data/cifar-10-batches-py/data_batch_%d\"%(i))\n",
    "    if len(x_train)==0 & len(y_train)==0:\n",
    "        x_train = batch[b'data']\n",
    "        y_train = batch[b'labels']\n",
    "    else:\n",
    "        x_train = np.concatenate((x_train, batch[b'data'])) \n",
    "        y_train = np.concatenate((y_train, batch[b'labels']))\n",
    "\n",
    "v_batch = unpickle(\"data/cifar-10-batches-py/data_batch_5\")\n",
    "x_val = v_batch[b'data']\n",
    "y_val = v_batch[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train0 = np.empty((0,10), int)\n",
    "for y in y_train:\n",
    "    indi= [1 if i==y else 0 for i in range(0,10)]\n",
    "    y_train0 = np.append(y_train0, np.array([indi]), axis=0)\n",
    "y_val0 = np.empty((0,10), int)\n",
    "for y in y_val:\n",
    "    indi= [1 if i==y else 0 for i in range(0,10)]\n",
    "    y_val0 = np.append(y_val0, np.array([indi]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_batch = unpickle(\"data/cifar-10-batches-py/test_batch\")\n",
    "x_test = test_batch[b'data']\n",
    "y_test = test_batch[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test0 = np.empty((0,10), int)\n",
    "for y in y_test:\n",
    "    indi= [1 if i==y else 0 for i in range(0,10)]\n",
    "    y_test0 = np.append(y_test0, np.array([indi]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Build The Reduced_ALLCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_logits(x):\n",
    "    \"\"\"CNN-FMP network model\"\"\"\n",
    "    \n",
    "    #input x.shape = [,32*32*3]\n",
    "    x_image = tf.reshape(x,[-1,32,32,3])\n",
    "    \n",
    "    #define some constants, stack size for convolutional layers\n",
    "    n1=32\n",
    "    n2=64\n",
    "    n3=96\n",
    "    n4=96\n",
    " \n",
    "\n",
    "    \n",
    "    #Block1\n",
    "    #cnn_1\n",
    "    W_conv1 = tf.get_variable('W_conv1', shape=[2, 2, 3, n1])\n",
    "    b_conv1 = tf.get_variable('b_conv1', shape=[n1])\n",
    "    h_conv1 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME'), b_conv1))\n",
    "    #cnn_2\n",
    "    W_conv2 = tf.get_variable('W_conv2', shape=[2, 2, n1, n1])\n",
    "    b_conv2 = tf.get_variable('b_conv2', shape=[n1])\n",
    "    h_conv2 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(h_conv1, W_conv2, strides=[1, 1, 1, 1], padding='SAME'), b_conv2))\n",
    "    #cnn_3\n",
    "    W_conv3 = tf.get_variable('W_conv3', shape=[2, 2, n1, n1])\n",
    "    b_conv3 = tf.get_variable('b_conv3', shape=[n1])\n",
    "    h_conv3 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(h_conv2, W_conv3, strides=[1, 2, 2, 1], padding='SAME'), b_conv3))\n",
    "    \n",
    "\n",
    "    \n",
    "    #block_2\n",
    "    #dropout rate = 0.1 \n",
    "    #cnn_4\n",
    "    W_conv4 = tf.get_variable('W_conv4', shape=[2, 2, n1, n2])\n",
    "    b_conv4 = tf.get_variable('b_conv4', shape=[n2])\n",
    "    h_conv4 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='SAME'), b_conv4))\n",
    "    #dropout_1\n",
    "    d_1=tf.nn.dropout(h_conv4,0.9)\n",
    "    #cnn_5\n",
    "    W_conv5 = tf.get_variable('W_conv5', shape=[2, 2, n2, n2])\n",
    "    b_conv5 = tf.get_variable('b_conv5', shape=[n2])\n",
    "    h_conv5 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_1, W_conv5, strides=[1, 1, 1, 1], padding='SAME'), b_conv5))\n",
    "    #dropout_2\n",
    "    d_2=tf.nn.dropout(h_conv5,0.9)\n",
    "    #cnn_6\n",
    "    W_conv6 = tf.get_variable('W_conv6', shape=[2, 2, n2, n2])\n",
    "    b_conv6 = tf.get_variable('b_conv6', shape=[n2])\n",
    "    h_conv6 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_2, W_conv6, strides=[1, 2, 2, 1], padding='SAME'), b_conv6))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #block_3\n",
    "    #dropout=0.2\n",
    "    #cnn_7\n",
    "    W_conv7 = tf.get_variable('W_conv7', shape=[2, 2, n2, n3])\n",
    "    b_conv7 = tf.get_variable('b_conv7', shape=[n3])\n",
    "    h_conv7 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(h_conv6, W_conv7, strides=[1, 1, 1, 1], padding='SAME'), b_conv7))\n",
    "    #dropout_3\n",
    "    d_3=tf.nn.dropout(h_conv7,0.8)\n",
    "    #cnn_8\n",
    "    W_conv8 = tf.get_variable('W_conv8', shape=[2, 2, n3, n3])\n",
    "    b_conv8 = tf.get_variable('b_conv8', shape=[n3])\n",
    "    h_conv8 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_3, W_conv8, strides=[1, 1, 1, 1], padding='SAME'), b_conv8))\n",
    "    #dropout_4\n",
    "    d_4=tf.nn.dropout(h_conv8,0.8)\n",
    "    #cnn_9\n",
    "    W_conv9 = tf.get_variable('W_conv9', shape=[2, 2, n3, n3])\n",
    "    b_conv9 = tf.get_variable('b_conv9', shape=[n3])\n",
    "    h_conv9 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_4, W_conv9, strides=[1, 2, 2, 1], padding='SAME'), b_conv9))\n",
    "    \n",
    "\n",
    "    \n",
    "    #block_4\n",
    "    #dropout=0.3\n",
    "    #cnn_10\n",
    "    W_conv10 = tf.get_variable('W_conv10', shape=[2, 2, n3, n4])\n",
    "    b_conv10 = tf.get_variable('b_conv10', shape=[n4])\n",
    "    h_conv10 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(h_conv9, W_conv10, strides=[1, 1, 1, 1], padding='SAME'), b_conv10))\n",
    "    #dropout_5\n",
    "    d_5=tf.nn.dropout(h_conv10,0.7)\n",
    "    #cnn_11\n",
    "    W_conv11 = tf.get_variable('W_conv11', shape=[2, 2, n4, n4])\n",
    "    b_conv11 = tf.get_variable('b_conv11', shape=[n4])\n",
    "    h_conv11 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_5, W_conv11, strides=[1, 1, 1, 1], padding='SAME'), b_conv11))\n",
    "    #dropout_6\n",
    "    d_6=tf.nn.dropout(h_conv11,0.7)\n",
    "    #cnn_12\n",
    "    W_conv12 = tf.get_variable('W_conv12', shape=[2, 2, n4, n4])\n",
    "    b_conv12 = tf.get_variable('b_conv12', shape=[n4])\n",
    "    h_conv12 = tf.nn.leaky_relu(tf.add(tf.nn.conv2d(d_5, W_conv12, strides=[1, 2, 2, 1], padding='SAME'), b_conv12))\n",
    "    \n",
    "\n",
    "\n",
    "    # fc layer to logits 10\n",
    "    h_flat = tf.reshape(h_conv12, [-1, 2*2*n4])\n",
    "    W_fc1 = tf.get_variable('W_fc1', shape=[2*2*n4, 10])\n",
    "    b_fc1 = tf.get_variable('b_fc1', shape=[10])\n",
    "    #layer of output\n",
    "    logits = tf.add(tf.matmul(h_flat, W_fc1), b_fc1, name='logits')\n",
    "    \n",
    "    return(logits)\n",
    "\n",
    "def compute_cross_entropy(logits, y):\n",
    "    \"\"\"compute the prediction and cross_entropy of model\"\"\"\n",
    "    # This function is used from the in-class example code\n",
    "    numerical_instability_example = 0\n",
    "    if numerical_instability_example:\n",
    "        y_pred = tf.nn.softmax(logits, name='y_pred') \n",
    "        cross_ent = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred), reduction_indices=[1]))\n",
    "    else:\n",
    "        sm_ce = tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits, name='cross_ent_terms')\n",
    "        cross_ent = tf.reduce_mean(sm_ce, name='cross_ent')\n",
    "    return cross_ent\n",
    "\n",
    "def compute_accuracy(logits, y):\n",
    "    \"compare prediction to labels\"\n",
    "    prediction = tf.argmax(logits, 1, name='pred_class')\n",
    "    true_label = tf.argmax(y, 1, name='true_class')\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, true_label), tf.float32))\n",
    "    return accuracy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Train, Validate and Test the Reduced_ALLCNN Model\n",
    "In this part, we also save all the summaries, which are displayed in Tensorboard.\n",
    "We train the model using small batches(100) due to the GPU capacity. Also, for validation set, we validate model using a 1000 random sample from the validation 10000 data set for the same reason. Test accuracy is calculated using batches of 1000 and their average is calculated below. This result is for models comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After step   0, valiation accuracy 0.1002\n",
      "After step 1000, valiation accuracy 0.3761\n",
      "After step 2000, valiation accuracy 0.4217\n",
      "After step 3000, valiation accuracy 0.4272\n",
      "After step 4000, valiation accuracy 0.4449\n",
      "After step 5000, valiation accuracy 0.4407\n",
      "After step 6000, valiation accuracy 0.4349\n",
      "After step 7000, valiation accuracy 0.4408\n",
      "After step 8000, valiation accuracy 0.4386\n",
      "After step 9000, valiation accuracy 0.4360\n",
      "After step 10000, valiation accuracy 0.4361\n",
      "Final test accuracy is 0.4328\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'logs/scratch04x/summary'\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, 32*32*3], name='x')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='y')\n",
    "\n",
    "    \n",
    " \n",
    "    with tf.name_scope('model'):\n",
    "        logits = compute_logits(x)\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = compute_cross_entropy(logits=logits, y=y)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = compute_accuracy(logits, y)\n",
    "    \n",
    "    with tf.name_scope('opt'):\n",
    "\n",
    "        opt = tf.train.AdamOptimizer(1e-4)\n",
    "        train_step = opt.minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('summaries'):\n",
    "        # create summary for loss and accuracy\n",
    "        tf.summary.scalar('loss', loss) \n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        # create summary for logits\n",
    "        tf.summary.histogram('logits', logits)\n",
    "        # create summary for input image\n",
    "        tf.summary.image('input', tf.reshape(x, [-1, 32, 32, 3]))\n",
    "    \n",
    "        summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        summary_writer = tf.summary.FileWriter(dir_name, sess.graph)\n",
    "        summary_writer_train = tf.summary.FileWriter(dir_name+'/train', sess.graph)\n",
    "        summary_writer_test = tf.summary.FileWriter(dir_name+'/test')\n",
    "        summary_writer_val = tf.summary.FileWriter(dir_name+'/val')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        batch=0\n",
    "        for i in range(10001):\n",
    "            \n",
    "             \n",
    "            _ , summary = sess.run((train_step, summary_op),\n",
    "                                feed_dict={x: x_train[100*batch:100*(batch+1)], y: y_train0[100*batch:100*(batch+1)]})\n",
    "            batch=batch+1\n",
    "            if batch == 50:\n",
    "                batch=0\n",
    "            if i%10==0:\n",
    "                #t=rd.sample(range(0,10000),10000)\n",
    "                summary_writer_train.add_summary(summary, i)\n",
    "                (val_ac, summary_t) = sess.run((accuracy,summary_op), {x:x_val, y:y_val0})\n",
    "                summary_writer_val.add_summary(summary_t, i)\n",
    "                if i%1000 == 0:\n",
    "                    print(\"\\rAfter step {0:3d}, valiation accuracy {1:0.4f}\".format(i, val_ac), flush=True)\n",
    "        \n",
    "        test_ac=[]\n",
    "        for i in range(0,10):\n",
    "            acu=sess.run(accuracy,{x:x_test[1000*i:1000*(i+1)],y:y_test0[1000*i:1000*(i+1)]})\n",
    "            test_ac.append(acu)\n",
    "            \n",
    "        all_ac=sum(test_ac)*0.1\n",
    "        print(\"\\rFinal test accuracy is %.4f\"%all_ac)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ###\n",
    "According to the validation accuracy and test accuracy, it seems that reduced_ALLCNN model doesn't have a better performance. We reduced the filter size to prevent overfitting, but there is no improvement on the performance. So naively reducing layers is not an effective way to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
